{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube01-Psy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID  \\\n",
       "345      z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346  z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347    z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348    z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349  z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dvec = vectorizer.fit_transform(d['CONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<350x1418 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4354 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first billion viewed this because they thought it was really cool, the  other billion and a half came to see how stupid the first billion were...﻿\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'first',\n",
       " 'billion',\n",
       " 'viewed',\n",
       " 'this',\n",
       " 'because',\n",
       " 'they',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'was',\n",
       " 'really',\n",
       " 'cool',\n",
       " 'the',\n",
       " 'other',\n",
       " 'billion',\n",
       " 'and',\n",
       " 'half',\n",
       " 'came',\n",
       " 'to',\n",
       " 'see',\n",
       " 'how',\n",
       " 'stupid',\n",
       " 'the',\n",
       " 'first',\n",
       " 'billion',\n",
       " 'were']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d['CONTENT'][349])\n",
    "analyze(d['CONTENT'][349])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '034',\n",
       " '05',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '100000415527985',\n",
       " '10200253113705769',\n",
       " '1030',\n",
       " '1073741828',\n",
       " '11',\n",
       " '1111',\n",
       " '112720997191206369631',\n",
       " '12',\n",
       " '123',\n",
       " '124',\n",
       " '124923004',\n",
       " '126',\n",
       " '127',\n",
       " '13017194',\n",
       " '131338190916',\n",
       " '1340488',\n",
       " '1340489',\n",
       " '1340490',\n",
       " '1340491',\n",
       " '1340492',\n",
       " '1340493',\n",
       " '1340494',\n",
       " '1340499',\n",
       " '1340500',\n",
       " '1340502',\n",
       " '1340503',\n",
       " '1340504',\n",
       " '1340517',\n",
       " '1340518',\n",
       " '1340519',\n",
       " '1340520',\n",
       " '1340521',\n",
       " '1340522',\n",
       " '1340523',\n",
       " '1340524',\n",
       " '134470083389909',\n",
       " '1415297812',\n",
       " '1495323920744243',\n",
       " '1496241863981208',\n",
       " '1496273723978022',\n",
       " '1498561870415874',\n",
       " '161620527267482',\n",
       " '171183229277',\n",
       " '19',\n",
       " '19924',\n",
       " '1firo',\n",
       " '1m',\n",
       " '20',\n",
       " '2009',\n",
       " '2012',\n",
       " '2012bitches',\n",
       " '2013',\n",
       " '2014',\n",
       " '201470069872822',\n",
       " '2015',\n",
       " '2017',\n",
       " '210',\n",
       " '23',\n",
       " '24',\n",
       " '24398',\n",
       " '243a',\n",
       " '279',\n",
       " '29',\n",
       " '2b',\n",
       " '2billion',\n",
       " '2x10',\n",
       " '300',\n",
       " '3000',\n",
       " '313327',\n",
       " '315',\n",
       " '322',\n",
       " '33',\n",
       " '33gxrf',\n",
       " '39',\n",
       " '390875584405933',\n",
       " '391725794320912',\n",
       " '40beuutvu2zkxk4utgpz8k',\n",
       " '4436607',\n",
       " '4604617',\n",
       " '48051',\n",
       " '484',\n",
       " '492',\n",
       " '4shared',\n",
       " '4snjqp',\n",
       " '4th',\n",
       " '50',\n",
       " '521',\n",
       " '5242575',\n",
       " '5277478',\n",
       " '5287',\n",
       " '57',\n",
       " '58',\n",
       " '5800',\n",
       " '5af506e1',\n",
       " '5c2f',\n",
       " '5million',\n",
       " '5s',\n",
       " '616375350',\n",
       " '636',\n",
       " '6381501',\n",
       " '694',\n",
       " '700',\n",
       " '733634264',\n",
       " '733949243353321',\n",
       " '734237113324534',\n",
       " '74',\n",
       " '750',\n",
       " '783',\n",
       " '79',\n",
       " '821',\n",
       " '884',\n",
       " '898',\n",
       " '8bit',\n",
       " '9082175',\n",
       " '9107',\n",
       " '9277547',\n",
       " '950',\n",
       " '969',\n",
       " '9bzkp7q19f0',\n",
       " '_0f9fa8aa',\n",
       " '__killuminati94',\n",
       " '_bzszz',\n",
       " '_chris_cz',\n",
       " '_fphgk5zllsvdqv0zuf0mb',\n",
       " '_gibu',\n",
       " '_o3h',\n",
       " '_ry6f57sprnd2xv',\n",
       " '_thqbeum69aqup1ih',\n",
       " '_trksid',\n",
       " '_vlczzrg8vgctlpsd9ongewhj8',\n",
       " 'aaaaaaa',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'access',\n",
       " 'accessories',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'acn2g',\n",
       " 'acting',\n",
       " 'active',\n",
       " 'actor',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'admit',\n",
       " 'adsense',\n",
       " 'advice',\n",
       " 'affiliateid',\n",
       " 'after',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'ahhh',\n",
       " 'al',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allways',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'alot',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'america',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'and',\n",
       " 'andrijamatf',\n",
       " 'android',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'animations',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyway',\n",
       " 'apocalypse',\n",
       " 'app',\n",
       " 'apparel',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'around',\n",
       " 'art',\n",
       " 'as',\n",
       " 'aseris',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aspx',\n",
       " 'ass',\n",
       " 'at',\n",
       " 'attention',\n",
       " 'auburn',\n",
       " 'audiojungle',\n",
       " 'auditioning',\n",
       " 'avaaz',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'b00ecvf93g',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'bass',\n",
       " 'bd3721315',\n",
       " 'be',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beibs',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bf4',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bighit',\n",
       " 'bilion',\n",
       " 'billion',\n",
       " 'billions',\n",
       " 'billon',\n",
       " 'binbox',\n",
       " 'bing',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'blanc',\n",
       " 'block',\n",
       " 'blue',\n",
       " 'bomb',\n",
       " 'book',\n",
       " 'bother',\n",
       " 'bots',\n",
       " 'bottom',\n",
       " 'bowl',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'bps',\n",
       " 'br',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brew',\n",
       " 'bringing',\n",
       " 'brother',\n",
       " 'brotherhood',\n",
       " 'brothers',\n",
       " 'brt0u5',\n",
       " 'bs',\n",
       " 'bubblews',\n",
       " 'bucket',\n",
       " 'burned',\n",
       " 'but',\n",
       " 'butalabs',\n",
       " 'buy',\n",
       " 'by',\n",
       " 'c349',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cant',\n",
       " 'capitalized',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'caroline',\n",
       " 'cash',\n",
       " 'cats',\n",
       " 'cd92db3f4',\n",
       " 'censor',\n",
       " 'certain',\n",
       " 'chacking',\n",
       " 'chainise',\n",
       " 'challenges',\n",
       " 'chance',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'chanicka',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'chhanel',\n",
       " 'chick',\n",
       " 'child',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'ching',\n",
       " 'chiptunes',\n",
       " 'christmas',\n",
       " 'chubbz',\n",
       " 'chuck',\n",
       " 'cirus',\n",
       " 'citizen',\n",
       " 'cjfoftxeba',\n",
       " 'cleaning',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'co',\n",
       " 'code',\n",
       " 'codes',\n",
       " 'codytolleson',\n",
       " 'college',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comentars',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comment_id',\n",
       " 'commenting',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'company',\n",
       " 'complaining',\n",
       " 'completely',\n",
       " 'concerts',\n",
       " 'condition',\n",
       " 'confidence',\n",
       " 'confirmed',\n",
       " 'connected',\n",
       " 'constructive',\n",
       " 'content',\n",
       " 'cool',\n",
       " 'could',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'cover',\n",
       " 'covers',\n",
       " 'craft',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'crdits',\n",
       " 'crea',\n",
       " 'credit',\n",
       " 'crew',\n",
       " 'criticism',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'cs',\n",
       " 'csgo',\n",
       " 'curled',\n",
       " 'cute',\n",
       " 'cvhmklt',\n",
       " 'cxpzpgb',\n",
       " 'czfcxsn0jnq',\n",
       " 'da',\n",
       " 'daaaaaaaaaaannng',\n",
       " 'dad',\n",
       " 'dafuq',\n",
       " 'daily',\n",
       " 'dance',\n",
       " 'dances',\n",
       " 'dancing',\n",
       " 'day',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'dedicated',\n",
       " 'defuse',\n",
       " 'deserve',\n",
       " 'designs',\n",
       " 'details',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'diddle',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'difference',\n",
       " 'difficult',\n",
       " 'dinero',\n",
       " 'ding',\n",
       " 'direction',\n",
       " 'disappointed',\n",
       " 'discover',\n",
       " 'dislike',\n",
       " 'dislikes',\n",
       " 'dislikesssssssssssssssssssssssssssssssss',\n",
       " 'divertenti',\n",
       " 'diys',\n",
       " 'dizzy',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'dolacz',\n",
       " 'dominate',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'dresses',\n",
       " 'dressprettyonce',\n",
       " 'driving',\n",
       " 'drone',\n",
       " 'drones',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'drunk',\n",
       " 'dubstep',\n",
       " 'dumb',\n",
       " 'dunno',\n",
       " 'during',\n",
       " 'duty',\n",
       " 'dylan',\n",
       " 'earn',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'easypromosapp',\n",
       " 'ebay',\n",
       " 'ede05ea397ca',\n",
       " 'editor',\n",
       " 'edm',\n",
       " 'eeccon',\n",
       " 'effects',\n",
       " 'effort',\n",
       " 'ehi',\n",
       " 'ejw9kvkoxdamqm808h5z',\n",
       " 'elevator',\n",
       " 'eliminate',\n",
       " 'emerson_zanol',\n",
       " 'end',\n",
       " 'english',\n",
       " 'enimen',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'enter',\n",
       " 'entertaining',\n",
       " 'entire',\n",
       " 'epic',\n",
       " 'equals',\n",
       " 'ermail',\n",
       " 'esteem',\n",
       " 'etc',\n",
       " 'eugenekalinin',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everyones',\n",
       " 'everything',\n",
       " 'ex',\n",
       " 'excuse',\n",
       " 'expectations',\n",
       " 'expensive',\n",
       " 'experiments',\n",
       " 'explore',\n",
       " 'exposure',\n",
       " 'eyebrows',\n",
       " 'eyes',\n",
       " 'f7',\n",
       " 'fablife',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'facts',\n",
       " 'fail',\n",
       " 'fake',\n",
       " 'family',\n",
       " 'fanboys',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fashionable',\n",
       " 'fb',\n",
       " 'featured',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'festival',\n",
       " 'few',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'fireball',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'flipagram',\n",
       " 'flute',\n",
       " 'fly',\n",
       " 'follow',\n",
       " 'follower',\n",
       " 'football',\n",
       " 'for',\n",
       " 'forget',\n",
       " 'foto',\n",
       " 'found',\n",
       " 'four',\n",
       " 'freddy',\n",
       " 'free',\n",
       " 'freemyapps',\n",
       " 'french',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frigea',\n",
       " 'from',\n",
       " 'fruity',\n",
       " 'fuck',\n",
       " 'fucked',\n",
       " 'fucken',\n",
       " 'fucking',\n",
       " 'fudairyqueen',\n",
       " 'fuego',\n",
       " 'fun',\n",
       " 'funnier',\n",
       " 'funny',\n",
       " 'funnytortspics',\n",
       " 'future',\n",
       " 'gabriel',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gamestop',\n",
       " 'gaming',\n",
       " 'ganga',\n",
       " 'gangam',\n",
       " 'gangman',\n",
       " 'gangnam',\n",
       " 'gangnamstyle',\n",
       " 'gatti',\n",
       " 'gay',\n",
       " 'gbphotographygb',\n",
       " 'gcmforex',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghost',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giveaways',\n",
       " 'giver',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'goes',\n",
       " 'gofundme',\n",
       " 'going',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'google',\n",
       " 'gook',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'gotten',\n",
       " 'government',\n",
       " 'gp',\n",
       " 'grateful',\n",
       " 'great',\n",
       " 'greetings',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grwmps',\n",
       " 'gt',\n",
       " 'gta5',\n",
       " 'guardalo',\n",
       " 'gun',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gvr7xg',\n",
       " 'gwar',\n",
       " 'hacked',\n",
       " 'hackers',\n",
       " 'hackfbaccountlive',\n",
       " 'had',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahahahah',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halftime',\n",
       " 'halp',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'hassle',\n",
       " 'hate',\n",
       " 'haters',\n",
       " 'hav',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'headbutt',\n",
       " 'hear',\n",
       " 'heart',\n",
       " 'heck',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hl',\n",
       " 'hole',\n",
       " 'holy',\n",
       " 'hop',\n",
       " 'hope',\n",
       " 'hoppa',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'html',\n",
       " 'http',\n",
       " 'https',\n",
       " 'huge',\n",
       " 'huh',\n",
       " 'humanity',\n",
       " 'hunger',\n",
       " 'hw',\n",
       " 'hwang',\n",
       " 'hyperurl',\n",
       " 'hyuna',\n",
       " 'ice',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ig',\n",
       " 'il',\n",
       " 'ill',\n",
       " 'illuminati',\n",
       " 'im',\n",
       " 'image2you',\n",
       " 'imagine',\n",
       " 'improve',\n",
       " 'in',\n",
       " 'including',\n",
       " 'incmedia',\n",
       " 'indiegogo',\n",
       " 'inspired',\n",
       " 'instagram',\n",
       " 'instagraml',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'inviting',\n",
       " 'io',\n",
       " 'iphone',\n",
       " 'iq2',\n",
       " 'irl',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'isnt',\n",
       " 'it',\n",
       " 'ithat',\n",
       " 'itm',\n",
       " 'its',\n",
       " 'itunes',\n",
       " 'itz',\n",
       " 'jackal',\n",
       " 'jackson',\n",
       " 'jae',\n",
       " 'james',\n",
       " 'jap',\n",
       " 'jaroadc',\n",
       " 'jb',\n",
       " 'jelly',\n",
       " 'jellyfish',\n",
       " 'jenny',\n",
       " 'job',\n",
       " 'join',\n",
       " 'joint2',\n",
       " 'joke',\n",
       " 'joking',\n",
       " 'jr',\n",
       " 'jtcjnmho',\n",
       " 'juice',\n",
       " 'julie',\n",
       " 'julien',\n",
       " 'juno',\n",
       " 'just',\n",
       " 'justin',\n",
       " 'juyk',\n",
       " 'jyp',\n",
       " 'k6a5xt',\n",
       " 'kamal',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'keyword',\n",
       " 'kickstarter',\n",
       " 'kids',\n",
       " 'kidsmediausa',\n",
       " 'kidz',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'kobyoshi02',\n",
       " 'kodysman',\n",
       " 'koean',\n",
       " 'kollektivet',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'koreans',\n",
       " 'kyle',\n",
       " 'l2649',\n",
       " 'l551h',\n",
       " 'la',\n",
       " 'lacked',\n",
       " 'lada',\n",
       " 'language',\n",
       " 'last',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'launchpad',\n",
       " 'lazy',\n",
       " 'leader',\n",
       " 'league',\n",
       " 'leah',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'lexis',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likes',\n",
       " 'liking',\n",
       " 'limit',\n",
       " 'ling',\n",
       " 'link',\n",
       " 'linkbucks',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'listing',\n",
       " 'lists',\n",
       " 'little',\n",
       " 'littlebrother',\n",
       " 'live',\n",
       " 'll',\n",
       " 'lnuj',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lool',\n",
       " 'loool',\n",
       " 'loops',\n",
       " 'lordviperas',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'loving',\n",
       " 'low',\n",
       " 'lt',\n",
       " 'lucas',\n",
       " 'lucks',\n",
       " 'luka1qmrhf',\n",
       " 'luther',\n",
       " 'lyrics',\n",
       " 'm1555',\n",
       " 'mabey',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'management',\n",
       " 'many',\n",
       " 'mario',\n",
       " 'marius',\n",
       " 'market',\n",
       " 'marketglory',\n",
       " 'markusmairhofer',\n",
       " 'martin',\n",
       " 'mathster',\n",
       " 'may',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'mee',\n",
       " 'meets',\n",
       " 'members',\n",
       " 'memories',\n",
       " 'men',\n",
       " 'mercury',\n",
       " 'meselx',\n",
       " 'michael',\n",
       " 'miley',\n",
       " 'milions',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'millioon',\n",
       " 'millisecond',\n",
       " 'millon',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minecraft',\n",
       " 'minoo',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'model',\n",
       " 'mom',\n",
       " 'mon',\n",
       " 'money',\n",
       " 'monkey',\n",
       " 'monkeys',\n",
       " 'montages',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morgage',\n",
       " 'moroccan',\n",
       " 'most',\n",
       " 'mother',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'mp3s',\n",
       " 'ms',\n",
       " 'mscalifornia95',\n",
       " 'msmarilynmiles',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'murdev',\n",
       " 'music',\n",
       " 'must',\n",
       " 'mute',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nail',\n",
       " 'nails',\n",
       " 'name',\n",
       " 'national',\n",
       " 'necks',\n",
       " 'need',\n",
       " 'neon',\n",
       " 'net',\n",
       " 'network',\n",
       " 'never',\n",
       " 'new',\n",
       " 'newest',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicushorbboy',\n",
       " 'night',\n",
       " 'nike',\n",
       " 'ninja',\n",
       " 'no',\n",
       " 'non',\n",
       " 'norrus',\n",
       " 'not',\n",
       " 'notch',\n",
       " 'now',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'obsessed',\n",
       " 'odowd',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'officialpsy',\n",
       " 'offset',\n",
       " 'offıcal',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'olds',\n",
       " 'oldspice',\n",
       " 'olp_tab_refurbished',\n",
       " 'omg',\n",
       " 'on',\n",
       " 'once',\n",
       " 'oncueapparel',\n",
       " 'one',\n",
       " 'only',\n",
       " 'oppa',\n",
       " 'or',\n",
       " 'org',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'ovbiously',\n",
       " 'over',\n",
       " 'own',\n",
       " 'p3984',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'paid',\n",
       " 'pal',\n",
       " 'pan',\n",
       " 'pants',\n",
       " 'part',\n",
       " 'partners',\n",
       " 'party',\n",
       " 'passed',\n",
       " 'pause',\n",
       " 'pay',\n",
       " 'pazzi',\n",
       " 'pdf',\n",
       " 'pe',\n",
       " 'peace',\n",
       " 'penis',\n",
       " 'people',\n",
       " 'peoples',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'permpage',\n",
       " 'person',\n",
       " 'petition',\n",
       " 'petitions',\n",
       " 'phenomena',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'php',\n",
       " 'pictures',\n",
       " 'piece',\n",
       " 'pivot',\n",
       " 'pl',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planet',\n",
       " 'platform',\n",
       " 'play',\n",
       " 'please',\n",
       " 'plizz',\n",
       " 'pls',\n",
       " 'plus',\n",
       " 'plz',\n",
       " 'pnref',\n",
       " 'po',\n",
       " 'point',\n",
       " 'pop',\n",
       " 'popaegis',\n",
       " 'popular',\n",
       " 'population',\n",
       " 'populatoin',\n",
       " 'portfolio',\n",
       " 'post',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'pouring',\n",
       " 'pplease',\n",
       " 'pray',\n",
       " 'praying',\n",
       " 'prehistoric',\n",
       " 'premium',\n",
       " 'pretty',\n",
       " 'preview',\n",
       " 'pride',\n",
       " 'problem',\n",
       " 'prod',\n",
       " 'producer',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'promise',\n",
       " ...]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dshuf = d.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_train = dshuf[:300]\n",
    "d_test = dshuf[300:]\n",
    "d_train_att = vectorizer.fit_transform(d_train['CONTENT']) # fit bag-of-words on training set\n",
    "d_test_att = vectorizer.transform(d_test['CONTENT']) # reuse on testing set\n",
    "d_train_label = d_train['CLASS']\n",
    "d_test_label = d_test['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<300x1287 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3737 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x1287 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 484 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=80, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(d_train_att, d_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(d_test_att, d_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  0],\n",
       "       [ 1, 22]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred_labels = clf.predict(d_test_att)\n",
    "confusion_matrix(d_test_label, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, d_train_att, d_train_label, cv=5)\n",
    "# show average score and +/- two standard deviations away (covering 95% of scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load all datasets and combine them\n",
    "d = pd.concat([pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube01-Psy.csv\"),\n",
    "               pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube02-KatyPerry.csv\"),\n",
    "               pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube03-LMFAO.csv\"),\n",
    "               pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube04-Eminem.csv\"),\n",
    "               pd.read_csv(\"YouTube-Spam-Collection-v1/Youtube05-Shakira.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dshuf = d.sample(frac=1)\n",
    "d_content = dshuf['CONTENT']\n",
    "d_label = dshuf['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bag-of-words', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "     ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up a pipeline\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bag-of-words', CountVectorizer()),\n",
    "    ('random forest', RandomForestClassifier()),\n",
    "])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or: pipeline = make_pipeline(CountVectorizer(), RandomForestClassifier())\n",
    "make_pipeline(CountVectorizer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bag-of-words', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "     ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(d_content[:1500],d_label[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94298245614035092"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(d_content[1500:], d_label[1500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"what a neat video!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict([\"plz subscribe to my channel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, d_content, d_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add tfidf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipeline2 = make_pipeline(CountVectorizer(),\n",
    "                          TfidfTransformer(norm=None),\n",
    "                          RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline2, d_content, d_label, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer',\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('tfidftransformer',\n",
       "  TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameter search\n",
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    'countvectorizer__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'countvectorizer__stop_words': ('english', None),\n",
    "    'tfidftransformer__use_idf': (True, False), # effectively turn on/off tfidf\n",
    "    'randomforestclassifier__n_estimators': (20, 50, 100)\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline2, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:   39.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "  ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'countvectorizer__max_features': (None, 1000, 2000), 'countvectorizer__ngram_range': ((1, 1), (1, 2)), 'countvectorizer__stop_words': ('english', None), 'tfidftransformer__use_idf': (True, False), 'randomforestclassifier__n_estimators': (20, 50, 100)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(d_content, d_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.958\n",
      "Best parameters set:\n",
      "\tcountvectorizer__max_features: 1000\n",
      "\tcountvectorizer__ngram_range: (1, 1)\n",
      "\tcountvectorizer__stop_words: 'english'\n",
      "\trandomforestclassifier__n_estimators: 100\n",
      "\ttfidftransformer__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
